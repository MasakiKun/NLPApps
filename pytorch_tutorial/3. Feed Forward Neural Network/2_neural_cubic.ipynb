{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with Cubic Data (multi-dimensional)\n",
    "- y = x^3 -3x^2 -9x -1\n",
    "- 5 hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from visdom import Visdom\n",
    "viz = Visdom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/NLPApps/lib/python3.7/site-packages/ipykernel_launcher.py:4: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
      "  after removing the cwd from sys.path.\n",
      "/anaconda3/envs/NLPApps/lib/python3.7/site-packages/ipykernel_launcher.py:7: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "num_data = 1000\n",
    "num_epoch = 5000\n",
    "\n",
    "x = init.uniform(torch.Tensor(num_data,1),-10,10)\n",
    "y = (x**3) - 3*(x**2) - 9*x - 1\n",
    "\n",
    "noise = init.normal(torch.FloatTensor(num_data,1),std=3)\n",
    "\n",
    "y_noise = y + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "\n",
    "input_data = torch.cat([x,y_noise],1)\n",
    "\n",
    "win=viz.scatter(\n",
    "    X = input_data,\n",
    "    opts=dict(\n",
    "        xtickmin=-10,\n",
    "        xtickmax=10,\n",
    "        xtickstep=1,\n",
    "        ytickmin=0,\n",
    "        ytickmax=500,\n",
    "        ytickstep=1,\n",
    "        markersymbol='dot',\n",
    "        markercolor=np.random.randint(0, 255, num_data),\n",
    "        markersize=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data\n",
    "\n",
    "input_data = torch.cat([x,y_noise],1)\n",
    "\n",
    "win=viz.scatter(\n",
    "    X = torch.cat([x,y],1),\n",
    "    opts=dict(\n",
    "        xtickmin=-10,\n",
    "        xtickmax=10,\n",
    "        xtickstep=1,\n",
    "        ytickmin=0,\n",
    "        ytickmax=500,\n",
    "        ytickstep=1,\n",
    "        markersymbol='dot',\n",
    "        markercolor=np.random.randint(0, 255, num_data),\n",
    "        markersize=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected model with 5 hidden layer\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(1,20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20,10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10,5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5,1),\n",
    "        )\n",
    "\n",
    "loss_func = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.0005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(204.4301, grad_fn=<L1LossBackward>)\n",
      "tensor(204.3575, grad_fn=<L1LossBackward>)\n",
      "tensor(204.2652, grad_fn=<L1LossBackward>)\n",
      "tensor(204.1372, grad_fn=<L1LossBackward>)\n",
      "tensor(203.9411, grad_fn=<L1LossBackward>)\n",
      "tensor(203.6156, grad_fn=<L1LossBackward>)\n",
      "tensor(203.0410, grad_fn=<L1LossBackward>)\n",
      "tensor(201.9357, grad_fn=<L1LossBackward>)\n",
      "tensor(199.3703, grad_fn=<L1LossBackward>)\n",
      "tensor(191.3913, grad_fn=<L1LossBackward>)\n",
      "tensor(160.1453, grad_fn=<L1LossBackward>)\n",
      "tensor(134.2509, grad_fn=<L1LossBackward>)\n",
      "tensor(133.2757, grad_fn=<L1LossBackward>)\n",
      "tensor(132.3921, grad_fn=<L1LossBackward>)\n",
      "tensor(131.4680, grad_fn=<L1LossBackward>)\n",
      "tensor(130.4908, grad_fn=<L1LossBackward>)\n",
      "tensor(129.4394, grad_fn=<L1LossBackward>)\n",
      "tensor(128.2592, grad_fn=<L1LossBackward>)\n",
      "tensor(126.9604, grad_fn=<L1LossBackward>)\n",
      "tensor(125.4518, grad_fn=<L1LossBackward>)\n",
      "tensor(123.6775, grad_fn=<L1LossBackward>)\n",
      "tensor(121.5244, grad_fn=<L1LossBackward>)\n",
      "tensor(118.7848, grad_fn=<L1LossBackward>)\n",
      "tensor(115.1153, grad_fn=<L1LossBackward>)\n",
      "tensor(110.0428, grad_fn=<L1LossBackward>)\n",
      "tensor(103.3723, grad_fn=<L1LossBackward>)\n",
      "tensor(95.4565, grad_fn=<L1LossBackward>)\n",
      "tensor(86.5854, grad_fn=<L1LossBackward>)\n",
      "tensor(77.8004, grad_fn=<L1LossBackward>)\n",
      "tensor(69.1633, grad_fn=<L1LossBackward>)\n",
      "tensor(61.4661, grad_fn=<L1LossBackward>)\n",
      "tensor(56.0262, grad_fn=<L1LossBackward>)\n",
      "tensor(57.3816, grad_fn=<L1LossBackward>)\n",
      "tensor(62.0114, grad_fn=<L1LossBackward>)\n",
      "tensor(59.3887, grad_fn=<L1LossBackward>)\n",
      "tensor(55.4593, grad_fn=<L1LossBackward>)\n",
      "tensor(57.9145, grad_fn=<L1LossBackward>)\n",
      "tensor(55.2183, grad_fn=<L1LossBackward>)\n",
      "tensor(56.2899, grad_fn=<L1LossBackward>)\n",
      "tensor(49.2990, grad_fn=<L1LossBackward>)\n",
      "tensor(48.3713, grad_fn=<L1LossBackward>)\n",
      "tensor(45.3428, grad_fn=<L1LossBackward>)\n",
      "tensor(42.1354, grad_fn=<L1LossBackward>)\n",
      "tensor(39.7876, grad_fn=<L1LossBackward>)\n",
      "tensor(42.8158, grad_fn=<L1LossBackward>)\n",
      "tensor(33.4373, grad_fn=<L1LossBackward>)\n",
      "tensor(40.7234, grad_fn=<L1LossBackward>)\n",
      "tensor(49.2182, grad_fn=<L1LossBackward>)\n",
      "tensor(33.0902, grad_fn=<L1LossBackward>)\n",
      "tensor(36.6449, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_arr =[]\n",
    "label = y_noise\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    output = model(x)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = loss_func(output,label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 ==0:\n",
    "        print(loss)\n",
    "        \n",
    "    #loss_arr.append(loss.cpu().data.numpy()[0])\n",
    "    loss_arr.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.7475],\n",
      "        [-0.4363],\n",
      "        [-0.2682],\n",
      "        [-0.5952],\n",
      "        [-0.6621],\n",
      "        [ 0.6841],\n",
      "        [-0.4795],\n",
      "        [-0.6447],\n",
      "        [ 1.0006],\n",
      "        [ 0.4622],\n",
      "        [ 0.4706],\n",
      "        [-0.5996],\n",
      "        [-0.5254],\n",
      "        [ 1.5467],\n",
      "        [-0.4452],\n",
      "        [ 0.2992],\n",
      "        [ 0.0456],\n",
      "        [ 0.2445],\n",
      "        [ 0.5043],\n",
      "        [-0.9219]], requires_grad=True), Parameter containing:\n",
      "tensor([-3.4015, -2.8000, -1.3406, -2.6527, -1.2210,  0.9777, -1.6541, -3.7057,\n",
      "         0.9133,  0.6792, -2.6207, -1.2887,  0.5238,  0.7543, -1.9942,  0.1557,\n",
      "        -0.6209, -0.7606, -1.0957, -1.6237], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.2295, -0.1259, -0.1149, -0.1376, -0.1407, -0.0087,  0.2154,  0.1190,\n",
      "          0.0798,  0.1097, -0.1709,  0.0085, -0.2210, -0.0897, -0.1071, -0.1565,\n",
      "         -0.0376, -0.0334,  0.0526,  0.2063],\n",
      "        [ 0.1049,  1.0123,  0.5155,  0.6560,  0.3726, -0.0309,  0.2900,  1.2921,\n",
      "         -0.1179,  0.0102, -0.1536,  0.3545, -0.1656, -0.1215,  0.4010,  0.0840,\n",
      "          0.0227,  0.0856,  0.1477,  0.4848],\n",
      "        [-0.2197,  1.0396,  0.5222,  0.5950,  0.3511, -0.2885,  0.4225,  1.2581,\n",
      "          0.1432, -0.1054, -0.2064,  0.2331, -0.3659,  0.1433,  0.6313,  0.0538,\n",
      "         -0.1557, -0.1995, -0.0478,  0.3353],\n",
      "        [-0.3296,  0.3638,  0.2402,  0.1232,  0.0989,  0.1305,  0.1304,  0.2421,\n",
      "          0.4913, -0.0470, -0.1927,  0.2495,  0.0720,  0.5071,  0.3814,  0.0808,\n",
      "         -0.0289, -0.1205, -0.1177,  0.3848],\n",
      "        [-2.3853,  1.5860,  0.5744,  1.2599,  0.5037,  0.5349,  0.8146,  1.8893,\n",
      "          0.5357,  0.2684, -2.0959,  0.6768, -0.0960,  0.5216,  1.0610,  0.0925,\n",
      "         -0.0827, -0.4303, -0.5248,  0.7434],\n",
      "        [-0.1099, -0.0766, -0.1726, -0.1611, -0.1865, -0.1349, -0.0653, -0.2055,\n",
      "         -0.1032, -0.1529,  0.0634,  0.1680,  0.1915,  0.0925,  0.2088, -0.1850,\n",
      "          0.0230, -0.1345, -0.1919, -0.0957],\n",
      "        [-0.7161,  0.5061,  0.1007,  0.6113,  0.4145,  0.2498,  0.3701,  0.5722,\n",
      "          0.2475,  0.2533, -0.6627,  0.0679,  0.1346,  0.3549,  0.1482, -0.0392,\n",
      "         -0.1468, -0.1493, -0.0178,  0.5196],\n",
      "        [ 1.3364, -0.2017, -0.0094,  0.0087, -0.1738,  0.4948,  0.1302,  0.0185,\n",
      "          0.6724,  0.4155,  0.6909,  0.0066, -0.0357,  1.0656, -0.1493,  0.0869,\n",
      "         -0.0319,  0.1776,  0.5265,  0.0884],\n",
      "        [ 0.0675,  0.0634, -0.1202,  0.2014,  0.1230, -0.1195, -0.1737,  0.1362,\n",
      "         -0.1914, -0.1662,  0.1905, -0.1975, -0.1257, -0.1968, -0.2066,  0.0334,\n",
      "         -0.1956, -0.0998,  0.0233, -0.1036],\n",
      "        [ 1.3578,  0.1549,  0.2012, -0.0688,  0.1425,  0.3927, -0.2586,  0.1397,\n",
      "          0.4502,  0.2846,  0.8443, -0.2098,  0.0497,  1.0831,  0.0101,  0.1364,\n",
      "          0.0037,  0.5205,  0.6827,  0.0156]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0011, -1.0199, -0.8954, -0.0412, -0.4474, -0.1975, -0.3008, -0.1766,\n",
      "         0.0867, -0.2232], requires_grad=True), Parameter containing:\n",
      "tensor([[-2.3867e-01,  2.0870e-01,  4.7102e-02, -3.5119e-02, -1.9205e-01,\n",
      "         -1.6530e-01, -1.3074e-01,  6.9541e-02,  1.2964e-01, -6.1896e-02],\n",
      "        [-4.7703e-02,  1.1700e-01, -2.5212e-01,  1.9418e-01, -2.4232e-01,\n",
      "         -1.1407e-01, -8.5068e-02, -5.4669e-02, -2.8613e-01,  2.7327e-02],\n",
      "        [-1.4586e-01,  2.1500e+00,  2.1518e+00,  8.3353e-01,  4.3134e+00,\n",
      "          2.2012e-01,  1.5287e+00, -6.3224e-01, -1.5761e-01, -7.6159e-01],\n",
      "        [-1.7492e-01, -7.3195e-02,  3.4886e-03,  5.7814e-01, -4.5363e-01,\n",
      "         -1.9935e-01,  1.7678e-01,  2.0274e+00,  1.2075e-01,  2.0520e+00],\n",
      "        [ 2.1757e-01, -3.0816e-01, -1.9125e-01,  2.3123e-01, -8.0504e-02,\n",
      "         -7.7782e-02,  1.7666e-01,  2.0353e-01, -1.0788e-02, -3.2025e-01]],\n",
      "       requires_grad=True), Parameter containing:\n",
      "tensor([-0.2359, -0.0034, -0.0454,  0.0492, -0.2332], requires_grad=True), Parameter containing:\n",
      "tensor([[-0.4195, -0.4024, -5.5595,  2.9741, -0.2914]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.3113], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "param_list = list(model.parameters())\n",
    "print(param_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Trained Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "win2=viz.scatter(\n",
    "    X = torch.cat([x, output],1),\n",
    "    opts=dict(\n",
    "        xtickmin=-10,\n",
    "        xtickmax=10,\n",
    "        xtickstep=1,\n",
    "        ytickmin=0,\n",
    "        ytickmax=500,\n",
    "        ytickstep=1,\n",
    "        markersymbol='dot',\n",
    "        markercolor=np.random.randint(0, 255, num_data),\n",
    "        markersize=5,\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Loss Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.reshape([i for i in range(num_epoch)],newshape=[num_epoch,1])\n",
    "loss_data = np.reshape(loss_arr,newshape=[num_epoch,1])\n",
    "\n",
    "win3=viz.line(\n",
    "    X = x,\n",
    "    Y = loss_data, \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
